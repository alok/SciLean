======================================================================
     SciLean Metal vs MLX vs PyTorch Benchmark Suite
======================================================================

MLX available:     True
PyTorch MPS:       True


----------------------------------------------------------------------
  GEMM (C = A @ B)
----------------------------------------------------------------------
Backend      | Size         |       Time    | GFLOP/s
----------------------------------------------------------------------
MLX          | 512x512x512  |      0.418 ms | 642.4 GFLOP/s
PyTorch MPS  | 512x512x512  |      0.438 ms | 612.9 GFLOP/s

MLX          | 1024x1024x1024 |      1.099 ms | 1954.1 GFLOP/s
PyTorch MPS  | 1024x1024x1024 |      0.477 ms | 4505.6 GFLOP/s

MLX          | 2048x2048x2048 |      2.472 ms | 6950.1 GFLOP/s
PyTorch MPS  | 2048x2048x2048 |      2.289 ms | 7505.9 GFLOP/s


----------------------------------------------------------------------
  Softmax
----------------------------------------------------------------------
Backend      | Size         |       Time    | GFLOP/s
----------------------------------------------------------------------
MLX          | 10000        |      0.131 ms | N/A
PyTorch MPS  | 10000        |      0.195 ms | N/A

MLX          | 100000       |      0.161 ms | N/A
PyTorch MPS  | 100000       |      0.259 ms | N/A

MLX          | 1000000      |      0.423 ms | N/A
PyTorch MPS  | 1000000      |      0.786 ms | N/A


----------------------------------------------------------------------
  Attention (single-head)
----------------------------------------------------------------------
Backend      | Size         |       Time    | GFLOP/s
----------------------------------------------------------------------
MLX          | 128x64       |      0.203 ms | 20.8 GFLOP/s
PyTorch MPS  | 128x64       |      0.265 ms | 15.9 GFLOP/s

MLX          | 256x64       |      0.270 ms | 62.4 GFLOP/s
PyTorch MPS  | 256x64       |      0.290 ms | 58.2 GFLOP/s

MLX          | 512x64       |      0.291 ms | 231.2 GFLOP/s
PyTorch MPS  | 512x64       |      0.349 ms | 192.8 GFLOP/s


----------------------------------------------------------------------
  ReduceSum
----------------------------------------------------------------------
Backend      | Size         |       Time    | GFLOP/s
----------------------------------------------------------------------
MLX          | 100000       |      0.144 ms | N/A
PyTorch MPS  | 100000       |      0.225 ms | N/A

MLX          | 1000000      |      0.147 ms | N/A
PyTorch MPS  | 1000000      |      0.246 ms | N/A

MLX          | 10000000     |      0.233 ms | N/A
PyTorch MPS  | 10000000     |      0.313 ms | N/A


----------------------------------------------------------------------
  SciLean Metal (from Lean executable)
----------------------------------------------------------------------
╔═══════════════════════════════════════════════════════════╗
║    Float32 (Native) vs Float64 (Conversion) Benchmark     ║
╚═══════════════════════════════════════════════════════════╝

Metal GPU: Available ✓

Float64: Lean stores double, Metal uses float → conversion overhead
Float32: Lean stores float via ByteArray → zero conversion

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
FILL OPERATION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  N=100000: Float64 0.519379ms, Float32 0.025746ms  (Float32 20.17x faster)
  N=1000000: Float64 0.127692ms, Float32 0.054671ms  (Float32 2.335x faster)
  N=10000000: Float64 0.890867ms, Float32 0.594521ms  (Float32 1.498x faster)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ADDITION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  N=100000: Float64 3.743608ms, Float32 0.067225ms  (Float32 55.68x faster)
  N=1000000: Float64 0.226037ms, Float32 0.169450ms  (Float32 1.333x faster)
  N=10000000: Float64 2.346329ms, Float32 1.333862ms  (Float32 1.759x faster)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
REDUCE SUM
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  N=100000: Float64 1.097500ms, Float32 0.073271ms  (Float32 14.97x faster)
  N=1000000: Float64 0.100513ms, Float32 0.087579ms  (Float32 1.147x faster)
  N=10000000: Float64 0.717817ms, Float32 0.764254ms  (Float64 1.064x faster)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
MATRIX MULTIPLY (GEMM) - Naive
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  256x256: Float64 1.1456ms (29.28 GFLOP/s), Float32 0.1437ms (233.4 GFLOP/s)
  512x512: Float64 0.3654ms (734.6 GFLOP/s), Float32 0.4313ms (622.3 GFLOP/s)
  1024x1024: Float64 2.1950ms (978.3 GFLOP/s), Float32 1.8654ms (1151. GFLOP/s)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
GEMM KERNEL COMPARISON: Naive vs Tiled vs Simdgroup vs MPS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
(MPS = Metal Performance Shaders, Apple's optimized library)

  256×256 (0.033 GFLOPS):
    Naive:   0.09234ms  363.37 GFLOP/s
    Tiled:   1.49430ms  22.454 GFLOP/s  (0.06x)
    Simd:    1.90651ms  17.599 GFLOP/s  (0.04x)
    SimdOpt: 2.78574ms  12.045 GFLOP/s  (0.03x)
    MPS:     11.5292ms  2.9103 GFLOP/s  (0.00x)

  512×512 (0.268 GFLOPS):
    Naive:   0.40556ms  661.87 GFLOP/s
    Tiled:   0.22005ms  1219.8 GFLOP/s  (1.84x)
    Simd:    0.17390ms  1543.5 GFLOP/s  (2.33x)
    SimdOpt: 0.17798ms  1508.2 GFLOP/s  (2.27x)
    MPS:     0.14433ms  1859.8 GFLOP/s  (2.80x)

  1024×1024 (2.147 GFLOPS):
    Naive:   0.80753ms  2659.3 GFLOP/s
    Tiled:   0.50832ms  4224.6 GFLOP/s  (1.58x)
    Simd:    0.40155ms  5347.8 GFLOP/s  (2.01x)
    SimdOpt: 0.38746ms  5542.3 GFLOP/s  (2.08x)
    MPS:     4.49905ms  477.31 GFLOP/s  (0.17x)

  2048×2048 (17.17 GFLOPS):
    Naive:   7.80341ms  2201.5 GFLOP/s
    Tiled:   2.64407ms  6497.4 GFLOP/s  (2.95x)
    Simd:    1.57439ms  10912. GFLOP/s  (4.95x)
    SimdOpt: 1.60414ms  10709. GFLOP/s  (4.86x)
    MPS:     1.06467ms  16136. GFLOP/s  (7.32x)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
M4-OPTIMIZED GEMM (requires 128-aligned sizes)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
(float4 loads, 128×128 tiles, no bounds checks)

  1024×1024:
    M4:   3.57942ms  599.95 GFLOP/s
    Simd: 0.65010ms  3303.2 GFLOP/s
    MPS:  0.44035ms  4876.7 GFLOP/s

  2048×2048:
    M4:   3.16633ms  5425.7 GFLOP/s
    Simd: 2.08910ms  8223.5 GFLOP/s
    MPS:  1.20530ms  14253. GFLOP/s

  4096×4096:
    M4:   11.0011ms  12493. GFLOP/s
    Simd: 8.83165ms  15562. GFLOP/s
    MPS:  6.05734ms  22689. GFLOP/s

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
LARGE MATRIX TEST
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  3072×3072: M4 11.57065ms (5011.1 GFLOP/s), MPS 7.181389ms (8073.9 GFLOP/s)
  4096×4096: M4 18.02720ms (7623.9 GFLOP/s), MPS 9.519847ms (14437. GFLOP/s)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
AXPY TEST (y = a*x + y) - all ByteArray, zero-copy FFI
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  N=100000: 0.492837ms (0.4058 GFLOP/s)
  N=1000000: 0.151608ms (13.191 GFLOP/s)
  N=10000000: 1.006308ms (19.874 GFLOP/s)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
MPS (GPU) vs ACCELERATE (CPU/AMX) COMPARISON
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
MPS: Metal Performance Shaders - runs on GPU
Accelerate: Apple's BLAS - runs on CPU with AMX coprocessor

  256×256:
    MPS (GPU):          0.247180ms  135.74 GFLOP/s
    Accelerate (AMX):   0.198667ms  168.89 GFLOP/s
    Winner: Accelerate (AMX)

  512×512:
    MPS (GPU):          0.349722ms  767.56 GFLOP/s
    Accelerate (AMX):   0.118028ms  2274.3 GFLOP/s
    Winner: Accelerate (AMX)

  1024×1024:
    MPS (GPU):          0.796167ms  2697.2 GFLOP/s
    Accelerate (AMX):   0.554570ms  3872.3 GFLOP/s
    Winner: Accelerate (AMX)

  2048×2048:
    MPS (GPU):          3.007208ms  5712.8 GFLOP/s
    Accelerate (AMX):   3.691306ms  4654.1 GFLOP/s
    Winner: MPS (GPU)

  4096×4096:
    MPS (GPU):          19.16226ms  7172.3 GFLOP/s
    Accelerate (AMX):   20.83847ms  6595.4 GFLOP/s
    Winner: MPS (GPU)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
FLASH ATTENTION (Single-Head)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
softmax(Q @ K^T / sqrt(d)) @ V

  seq=128, d=64: 7.824450ms (0.5360 GFLOP/s)
  seq=256, d=64: 8.708008ms (1.9266 GFLOP/s)
  seq=512, d=64: 15.37414ms (4.3650 GFLOP/s)
  seq=1024, d=64: 32.22638ms (8.3296 GFLOP/s)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
CAUSAL ATTENTION (masked)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  seq=128, d=64: 3.987746ms (0.5258 GFLOP/s)
  seq=256, d=64: 6.786842ms (1.2360 GFLOP/s)
  seq=512, d=64: 14.19370ms (2.3640 GFLOP/s)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
SOFTMAX (comparison with MLX/PyTorch)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  N=10000: 1.363975ms
  N=100000: 0.107171ms
  N=1000000: 0.201004ms

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Benchmark complete!
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Summary:
  MPS (GPU): Apple's Metal Performance Shaders library
  Accelerate (AMX): Apple's CPU BLAS using AMX coprocessor
  Flash Attention: Custom Metal kernel for transformer attention
  Both are highly optimized - comparing helps understand workload


======================================================================
  Benchmark Complete
======================================================================

To compare directly:
  MLX/PyTorch: This script
  SciLean Metal: lake build Float32Benchmark && .lake/build/bin/Float32Benchmark
